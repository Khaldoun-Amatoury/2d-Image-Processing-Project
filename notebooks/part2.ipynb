{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "```\n",
    "2DIP_exercise/\n",
    "│-- data/             # Contains images & videos\n",
    "│   │-- input/        # 1 image and 1 video for each phase respectively\n",
    "│   │-- output/       # All output images/videos must be stored here\n",
    "│-- notebooks/        # Jupyter Notebooks for each phase\n",
    "│   │-- part1.ipynb   # Image processing & feature extraction\n",
    "│   │-- part2.ipynb   # Optical flow, object detection and tracking \n",
    "│-- README.md         # Project instructions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "inputs = os.path.join(base_path, 'data','input')\n",
    "outputs = os.path.join(base_path, 'data','output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary Code for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(image):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(video_path):\n",
    "    # Re-open the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert BGR to RGB for matplotlib\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['animation.embed_limit'] = 100\n",
    "\n",
    "def display_video(video_path):\n",
    "    \n",
    "    frames = get_frames(video_path)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    im = ax.imshow(np.zeros_like(frames[0]))\n",
    "    ax.axis('off')\n",
    "\n",
    "    def update(frame):\n",
    "        im.set_array(frame)\n",
    "        return [im]\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update, frames=frames, interval=50, blit=True, repeat=False)\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Analyze movement patterns in a video sequence. **(6)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Compute dense optical flow for each frame in a video of a moving crowd. **(2)**\n",
    "\n",
    "b) Visualize the movement patterns in 2 different ways. **(2+2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optical_flow(video_path, hsv_output, arrow_output):\n",
    "    \"\"\"\n",
    "    Dense Farnebäck optical flow with two visualisations:\n",
    "    1) HSV color-wheel encoding\n",
    "    2) Arrow Overlay\n",
    "    \"\"\"\n",
    "    \n",
    "    #cap will feed us the frames, if the file does not open or does not exist, then the code will raise and I/O Exception Error.\n",
    "    cap = cv2.VideoCapture(video_path)                      # opens the input file\n",
    "    if not cap.isOpened():                                  # it checks if the file is not open\n",
    "        raise IOError(f\"Cannot Open {video_path}\")          # Exception Error\n",
    "    \n",
    "    # Copy the FPS/size from the source so the output matches perfectly.\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')                # .mp4 output\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)                         # gives you the framerate of the video\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))              # gives you the frame width\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))             # gives you the frame heignt\n",
    "\n",
    "    # For the two seperate files: One will hole the HSV frames, the other arrow overlays.\n",
    "    hsv_writer = cv2.VideoWriter(hsv_output, fourcc, fps, (w,h))\n",
    "    arrow_writer = cv2.VideoWriter(arrow_output, fourcc, fps, (w,h))\n",
    "\n",
    "    # Optical flow always compares \"previous vs current\" gray frames. If the file doesnot exist, the code will raise a Value Exception Error.\n",
    "    ok, prev = cap.read()                                   # grabs frame 0\n",
    "    if not ok:                                              # Handling exeption\n",
    "        raise ValueError(\"Is the video empty?\")\n",
    "    \n",
    "    prev_g = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)         # converts to grayscale\n",
    "\n",
    "    # This loop is used for looping the entire video. Standard OpenCV reads the loop: stops when the read fails\n",
    "    while True:                                             # While it is true\n",
    "        ok, frame = cap.read()                              # Read the next frame\n",
    "        if not ok:                                          # If cap.read is not ok\n",
    "            break                                           # Break the loop\n",
    "        \n",
    "        # Compute the dense Farneback flow between the two gray images.\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)      # grayscale version\n",
    "        flow = _dense_flow(prev_g, gray)                    #(u,v) field\n",
    "\n",
    "        # Turn the flow into two human-friendly pictures\n",
    "        hsv_img = _flow_to_hsv(flow)                        # color-wheel visual\n",
    "        arrow_img = _draw_arrows(frame, flow)               #arrow oerlay visual\n",
    "\n",
    "        hsv_writer.write(hsv_img)                           # append frame to hsv video\n",
    "        arrow_writer.write(arrow_img)                       # append frame to arrow video\n",
    "\n",
    "        # Advance the \"previous\" frame pointer for the next loop\n",
    "        prev_g = gray                                       # next iteration, current -> previous\n",
    "\n",
    "    # Always release captures/ writers so the OS finalises the .mp4 files\n",
    "    for wtr in (cap, hsv_writer, arrow_writer):             # close files, flush buffers\n",
    "        wtr.release()\n",
    "\n",
    "# Wrapper function around OpenCV's built in dense flow.\n",
    "def _dense_flow(prev_g, curr_g):\n",
    "    \"\"\"\n",
    "    Farnebäck dense optical-flow field (u,v) for two gray frames.\n",
    "    \"\"\"\n",
    "    return cv2.calcOpticalFlowFarneback(\n",
    "        prev_g, curr_g, None,\n",
    "        pyr_scale = 0.3, levels = 3,\n",
    "        winsize = 15, iterations = 3,\n",
    "        poly_n = 5, poly_sigma = 1.2,\n",
    "        flags = 0\n",
    "    )\n",
    "\n",
    "# Encodes each pixel's motion vector as hue (direction) and brightness (speed).\n",
    "def _flow_to_hsv(flow):\n",
    "    \"\"\"\n",
    "    Map flow -> HSV color wheel image (BGR for VideoWriter).\n",
    "    \"\"\"\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0],flow[..., 1])   # speed $ angle\n",
    "    hsv = np.zeros((*mag.shape, 3), dtype = np.uint8)       # empty hsv image\n",
    "    hsv[..., 0] = ang * 90 / np.pi                          #hue = direction (0-180 degree range)\n",
    "    hsv[..., 1] = 255                                       # fill saturation (vivid colors)\n",
    "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    print(\"cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR):\",cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR))\n",
    "    return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)             # back to BGR for saving\n",
    "\n",
    "# Produces an easy-to-read quiver pplot by subsampling the dense field.\n",
    "def _draw_arrows(frame, flow, step = 16, color = (0,255,0)):\n",
    "    \"\"\"\n",
    "    Overlay a sparse arrow grid on a copy of the frame.\n",
    "    \"\"\"\n",
    "    h,w = frame.shape[:2]                                   # frames dimensions\n",
    "    arrows = frame.copy()                                   # don't scribble on the original\n",
    "    for y in range(0, h, step):                             # row by row, every step pixels\n",
    "        for x in range(0, w, step):                         # column by column\n",
    "            dx, dy = flow[y, x].astype(int)                 # flow vector at grid point\n",
    "            cv2.arrowedLine(                                # draw a small green arrow\n",
    "                arrows, (x, y), (x + dx, y + dy),\n",
    "                color, 1, tipLength = 0.3\n",
    "            )\n",
    "    return arrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(inputs, 'part2.mp4')  # Replace with your input video path\n",
    "output_path1 = os.path.join(outputs, 'optical_flow_1.mp4')  # Output visualization video path\n",
    "output_path2 = os.path.join(outputs, 'optical_flow_2.mp4')  # Output visualization video path\n",
    "\n",
    "optical_flow(video_path, output_path1, output_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = display_video(output_path1)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = display_video(output_path2)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 : Identify and track a moving object in a video sequence. **(9)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Detect an object using template matching. The output would be the first frame where it appears, with a bounding box around the detected object. **(2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_object(video_path, template_path, output_path):\n",
    "    cap=cv2.VideoCapture(video_path)\n",
    "    template=cv2.imread(template_path, 0)\n",
    "    w,h=template.shape[::-1]   #r&C \n",
    "    method=cv2.TM_CCOEFF_NORMED #CorrCoeffNormed\n",
    "    threshold=0.9 \n",
    "    frame_count=0\n",
    "    detected_frame=None #flag\n",
    "\n",
    "    while cap.isOpened(): #ff\n",
    "        ret,frame=cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) #Gs\n",
    "        tmpm=cv2.matchTemplate(frame_gray,template,method) #tempMatch\n",
    "        _,max_val,_,max_loc=cv2.minMaxLoc(tmpm) #for best match\n",
    "    \n",
    "        if max_val>=threshold:\n",
    "            top_left=max_loc\n",
    "            bottom_right=(top_left[0]+w, top_left[1]+h)\n",
    "            cv2.rectangle(frame,top_left,bottom_right,(0,255,0),3) #draw\n",
    "            print(f\"Object detected in the frame {frame_count}\")\n",
    "            cv2.imwrite(output_path,frame)\n",
    "            detected_frame=frame.copy()  \n",
    "            break\n",
    "\n",
    "        frame_count +=1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "        \n",
    "    return detected_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(inputs, 'part2.mp4')  # Replace with your input video path\n",
    "template_path = os.path.join(inputs, 'template.png')  # Replace with your template image path\n",
    "output_path = os.path.join(outputs, 'detected_object.jpg')  # Output video path\n",
    "\n",
    "image = locate_object(video_path, template_path, output_path)\n",
    "display_images(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Implement a Kalman filter to predict the object's position in subsequent frames. **(5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track(video_path, template_path, output_path):\n",
    "    template=cv2.imread(template_path,0)\n",
    "    w,h= template.shape[::-1]          #inverse R&C \n",
    "    cap=cv2.VideoCapture(video_path)\n",
    "    method=cv2.TM_CCOEFF_NORMED  #tempm CorrCoeNormed\n",
    "    ret,frame=cap.read()\n",
    "\n",
    "    frame_height,frame_width=frame.shape[:2] #for the op 2channels\n",
    "    print('W :',frame_height,'H :',frame_width)\n",
    "    fps=cap.get(cv2.CAP_PROP_FPS)\n",
    "    fourcc=cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out=cv2.VideoWriter(output_path,fourcc,fps,(frame_width,frame_height))\n",
    " \n",
    "    kalman= cv2.KalmanFilter(4,2)  #4s 2m  using the inbuild kallman\n",
    "    kalman.transitionMatrix=np.array([[1,0,1,0],[0,1,0,1],[0,0,1,0],[0,0,0,1]],np.float32) #changes to nxt states\n",
    "\n",
    "    kalman.measurementMatrix= np.eye(2, 4, dtype=np.float32) #map the m to actual state\n",
    "    kalman.measurementMatrix= np.array([[1,0,0,0],[0,1,0,0]],np.float32) #mm\n",
    "\n",
    "    kalman.processNoiseCov=np.eye(4,dtype=np.float32)*0.8 #model trust\n",
    "\n",
    "    initialized= False #flag\n",
    "    frame_idx= 0\n",
    "    while cap.isOpened():           #ff\n",
    "        ret,frame=cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "    \n",
    "        frame_gray= cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #gs\n",
    "        tmpm=cv2.matchTemplate(frame_gray, template,method) #tempMatch using CrrCoeNor\n",
    "        print('Score of the frames based on Correlation Coefficient method:',tmpm) \n",
    "        _,_,max_val,max_loc= cv2.minMaxLoc(tmpm)\n",
    "        print('Max score in array is : ',max_val)\n",
    "\n",
    "        x,y =max_loc[0]+w//2,max_loc[1]+h//2\n",
    "        if not initialized:\n",
    "            kalman.statePre=np.array([[x],[y],[0],[0]], np.float32) #intial state guess\n",
    "            initialized= True\n",
    "        pred= kalman.predict()        # motion model included\n",
    "        mx,my= max_loc[0] + w//2, max_loc[1] + h//2 #recaluculate the m position\n",
    "        kalman.correct(np.array([[np.float32(mx)], [np.float32(my)]])) #correction\n",
    "\n",
    "        #draw \n",
    "        cv2.rectangle(frame,max_loc,(max_loc[0]+w,max_loc[1]+h),(0,255,0),2) #temp\n",
    "        cv2.circle(frame,(int(pred[0]),int(pred[1])),6,(0,0,255),-1) #predicted\n",
    "\n",
    "        out.write(frame)\n",
    "        frame_idx += 1\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(inputs, 'part2.mp4')  # Replace with your input video path\n",
    "template_path = os.path.join(inputs, 'template.png')  # Replace with your template image path\n",
    "output_path = os.path.join(outputs, 'tracked_object.mp4')  # Output video path\n",
    "\n",
    "track(video_path, template_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = display_video(output_path)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Compare Bayesian filtering and Kalman filtering (theoretically). **(2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO c): \n",
    "# Bayesian filters are broad class of algorithms that are used to compute probablity distribution of states given noisy measurements,\n",
    "# while Kalman filter are type of Bayesian filter that computes a point estimate(mean) and its covarience, not the whole distribution.\n",
    "# Since Bayesian filter estimates the full probablity distribution of states. That makes it computationally more expensive than Kalman fitler. \n",
    "# Kalman filter works well and efficeint only when linearity and gaussian noise is present, while Bayesian works for any type of noise and linearity/non-linearity.\n",
    "# The above condition makes Bayesian more flexible to implemnt in various application while Kalman filter is more suitable specific application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
