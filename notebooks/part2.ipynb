{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "```\n",
    "2DIP_exercise/\n",
    "│-- data/             # Contains images & videos\n",
    "│   │-- input/        # 1 image and 1 video for each phase respectively\n",
    "│   │-- output/       # All output images/videos must be stored here\n",
    "│-- notebooks/        # Jupyter Notebooks for each phase\n",
    "│   │-- part1.ipynb   # Image processing & feature extraction\n",
    "│   │-- part2.ipynb   # Optical flow, object detection and tracking \n",
    "│-- README.md         # Project instructions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "inputs = os.path.join(base_path, 'data','input')\n",
    "outputs = os.path.join(base_path, 'data','output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary Code for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(image):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(video_path):\n",
    "    # Re-open the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert BGR to RGB for matplotlib\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['animation.embed_limit'] = 100\n",
    "\n",
    "def display_video(video_path):\n",
    "    \n",
    "    frames = get_frames(video_path)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    im = ax.imshow(np.zeros_like(frames[0]))\n",
    "    ax.axis('off')\n",
    "\n",
    "    def update(frame):\n",
    "        im.set_array(frame)\n",
    "        return [im]\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update, frames=frames, interval=50, blit=True, repeat=False)\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Analyze movement patterns in a video sequence. **(6)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Compute dense optical flow for each frame in a video of a moving crowd. **(2)**\n",
    "\n",
    "b) Visualize the movement patterns in 2 different ways. **(2+2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optical_flow(video_path, output_path1, output_path2):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(inputs, 'part2.mp4')  # Replace with your input video path\n",
    "output_path1 = os.path.join(outputs, 'optical_flow_1.mp4')  # Output visualization video path\n",
    "output_path2 = os.path.join(outputs, 'optical_flow_2.mp4')  # Output visualization video path\n",
    "\n",
    "optical_flow(video_path, output_path1, output_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = display_video(output_path1)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = display_video(output_path2)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 : Identify and track a moving object in a video sequence. **(9)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Detect an object using template matching. The output would be the first frame where it appears, with a bounding box around the detected object. **(2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_object(video_path, template_path, output_path):\n",
    "    cap=cv2.VideoCapture(video_path)\n",
    "    template=cv2.imread(template_path, 0)\n",
    "    w,h=template.shape[::-1]   #r&C \n",
    "    method=cv2.TM_CCOEFF_NORMED #CorrCoeffNormed\n",
    "    threshold=0.9 \n",
    "    frame_count=0\n",
    "    detected_frame=None #flag\n",
    "\n",
    "    while cap.isOpened(): #ff\n",
    "        ret,frame=cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) #Gs\n",
    "        tmpm=cv2.matchTemplate(frame_gray,template,method) #tempMatch\n",
    "        _,max_val,_,max_loc=cv2.minMaxLoc(tmpm) #for best match\n",
    "    \n",
    "        if max_val>=threshold:\n",
    "            top_left=max_loc\n",
    "            bottom_right=(top_left[0]+w, top_left[1]+h)\n",
    "            cv2.rectangle(frame,top_left,bottom_right,(0,255,0),3) #draw\n",
    "            print(f\"Object detected in the frame {frame_count}\")\n",
    "            cv2.imwrite(output_path,frame)\n",
    "            detected_frame=frame.copy()  \n",
    "            break\n",
    "\n",
    "        frame_count +=1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "        \n",
    "    return detected_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(inputs, 'part2.mp4')  # Replace with your input video path\n",
    "template_path = os.path.join(inputs, 'template.png')  # Replace with your template image path\n",
    "output_path = os.path.join(outputs, 'detected_object.jpg')  # Output video path\n",
    "\n",
    "image = locate_object(video_path, template_path, output_path)\n",
    "display_images(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Implement a Kalman filter to predict the object's position in subsequent frames. **(5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track(video_path, template_path, output_path):\n",
    "    template=cv2.imread(template_path,0)\n",
    "    w,h= template.shape[::-1]          #inverse R&C \n",
    "    cap=cv2.VideoCapture(video_path)\n",
    "    method=cv2.TM_CCOEFF_NORMED  #tempm CorrCoeNormed\n",
    "    ret,frame=cap.read()\n",
    "\n",
    "    frame_height,frame_width=frame.shape[:2] #for the op 2channels\n",
    "    print('W :',frame_height,'H :',frame_width)\n",
    "    fps=cap.get(cv2.CAP_PROP_FPS)\n",
    "    fourcc=cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out=cv2.VideoWriter(output_path,fourcc,fps,(frame_width,frame_height))\n",
    " \n",
    "    kalman= cv2.KalmanFilter(4,2)  #4s 2m  using the inbuild kallman\n",
    "    kalman.transitionMatrix=np.array([[1,0,1,0],[0,1,0,1],[0,0,1,0],[0,0,0,1]],np.float32) #changes to nxt states\n",
    "\n",
    "    kalman.measurementMatrix= np.eye(2, 4, dtype=np.float32) #map the m to actual state\n",
    "    kalman.measurementMatrix= np.array([[1,0,0,0],[0,1,0,0]],np.float32) #mm\n",
    "\n",
    "    kalman.processNoiseCov=np.eye(4,dtype=np.float32)*0.8 #model trust\n",
    "\n",
    "    initialized= False #flag\n",
    "    frame_idx= 0\n",
    "    while cap.isOpened():           #ff\n",
    "        ret,frame=cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "    \n",
    "        frame_gray= cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #gs\n",
    "        tmpm=cv2.matchTemplate(frame_gray, template,method) #tempMatch using CrrCoeNor\n",
    "        _,_,_,max_loc= cv2.minMaxLoc(tmpm)\n",
    "\n",
    "        x,y =max_loc[0]+w//2,max_loc[1]+h//2\n",
    "        if not initialized:\n",
    "            kalman.statePre=np.array([[x],[y],[0],[0]], np.float32) #intial state guess\n",
    "            initialized= True\n",
    "        pred= kalman.predict()        # motion model included\n",
    "        mx,my= max_loc[0] + w//2, max_loc[1] + h//2 #recaluculate the m position\n",
    "        kalman.correct(np.array([[np.float32(mx)], [np.float32(my)]])) #correction\n",
    "\n",
    "        #draw \n",
    "        cv2.rectangle(frame,max_loc,(max_loc[0]+w,max_loc[1]+h),(0,255,0),2) #temp\n",
    "        cv2.circle(frame,(int(pred[0]),int(pred[1])),6,(0,0,255),-1) #predicted\n",
    "\n",
    "        out.write(frame)\n",
    "        frame_idx += 1\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(inputs, 'part2.mp4')  # Replace with your input video path\n",
    "template_path = os.path.join(inputs, 'template.png')  # Replace with your template image path\n",
    "output_path = os.path.join(outputs, 'tracked_object.mp4')  # Output video path\n",
    "\n",
    "track(video_path, template_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = display_video(output_path)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Compare Bayesian filtering and Kalman filtering (theoretically). **(2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO c):\n",
    "# Bayesian filters are general filtering frameworks that are used to compute probablity distribution of states given noisy measurements,\n",
    "# while Kalman filter are type of Bayesian filter that computes a point estimate(mean) and its covarience, not the whole distribution.\n",
    "# Since Bayesian filter estimates the full probablity distribution of states, that makes it computationally more expensive than Kalman fitler. \n",
    "# Kalman filter is fast and efficeint only when model has linearity and gaussian noise is present, while Bayesian works for any type of noise and linearity/non-linearity.\n",
    "# The above condition makes Bayesian more flexible to implemnt in various application while Kalman filter is more suitable specific application where there is linearity and fast tracking is needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rvt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
