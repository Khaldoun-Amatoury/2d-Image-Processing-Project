{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "```\n",
    "2DIP_exercise/\n",
    "│-- data/             # Contains images & videos\n",
    "│   │-- input/        # 1 image and 1 video for each phase respectively\n",
    "│   │-- output/       # All output images/videos must be stored here\n",
    "│-- notebooks/        # Jupyter Notebooks for each phase\n",
    "│   │-- part1.ipynb   # Image processing & feature extraction\n",
    "│   │-- part2.ipynb   # Optical flow, object detection and tracking \n",
    "│-- README.md         # Project instructions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "inputs = os.path.join(base_path, 'data','input')\n",
    "outputs = os.path.join(base_path, 'data','output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary Code for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(image):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(video_path):\n",
    "    # Re-open the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert BGR to RGB for matplotlib\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['animation.embed_limit'] = 100\n",
    "\n",
    "def display_video(video_path):\n",
    "    \n",
    "    frames = get_frames(video_path)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    im = ax.imshow(np.zeros_like(frames[0]))\n",
    "    ax.axis('off')\n",
    "\n",
    "    def update(frame):\n",
    "        im.set_array(frame)\n",
    "        return [im]\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update, frames=frames, interval=50, blit=True, repeat=False)\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Analyze movement patterns in a video sequence. **(6)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Compute dense optical flow for each frame in a video of a moving crowd. **(2)**\n",
    "\n",
    "b) Visualize the movement patterns in 2 different ways. **(2+2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optical_flow(video_path, hsv_output, arrow_output):\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    hsv_writer = cv2.VideoWriter(hsv_output, fourcc, fps, (w,h))\n",
    "    arrow_writer = cv2.VideoWriter(arrow_output, fourcc, fps, (w,h))\n",
    "    ok, prev = cap.read()\n",
    "    prev_g = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        flow = _dense_flow(prev_g, gray)\n",
    "        hsv_img = _flow_to_hsv(flow)\n",
    "        arrow_img = _draw_arrows(frame, flow)\n",
    "        hsv_writer.write(hsv_img)\n",
    "        arrow_writer.write(arrow_img)\n",
    "        prev_g = gray\n",
    " \n",
    "    for wtr in (cap, hsv_writer, arrow_writer):\n",
    "        wtr.release()\n",
    "\n",
    "def _dense_flow(prev_g, curr_g):\n",
    "    return cv2.calcOpticalFlowFarneback(\n",
    "        prev_g, curr_g, None,\n",
    "        pyr_scale = 0.3, levels = 3,\n",
    "        winsize = 15, iterations = 3,\n",
    "        poly_n = 5, poly_sigma = 1.2,\n",
    "        flags = 0\n",
    "    )\n",
    "\n",
    "def _flow_to_hsv(flow):\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0],flow[..., 1])\n",
    "    hsv = np.zeros((*mag.shape, 3), dtype = np.uint8)\n",
    "    hsv[..., 0] = ang * 90 / np.pi\n",
    "    hsv[..., 1] = 255\n",
    "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "def _draw_arrows(frame, flow, step = 16, color = (0,255,0)):\n",
    "    h,w = frame.shape[:2]\n",
    "    arrows = frame.copy()\n",
    "    for y in range(0, h, step):\n",
    "        for x in range(0, w, step):\n",
    "            dx, dy = flow[y, x].astype(int)\n",
    "            cv2.arrowedLine(\n",
    "                arrows, (x, y), (x + dx, y + dy),\n",
    "                color, 1, tipLength = 0.3\n",
    "            )\n",
    "    return arrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(inputs, 'part2.mp4')  # Replace with your input video path\n",
    "output_path1 = os.path.join(outputs, 'optical_flow_1.mp4')  # Output visualization video path\n",
    "output_path2 = os.path.join(outputs, 'optical_flow_2.mp4')  # Output visualization video path\n",
    "\n",
    "optical_flow(video_path, output_path1, output_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = display_video(output_path1)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = display_video(output_path2)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 : Identify and track a moving object in a video sequence. **(9)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Detect an object using template matching. The output would be the first frame where it appears, with a bounding box around the detected object. **(2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_object(video_path, template_path, output_path):\n",
    "    cap=cv2.VideoCapture(video_path)\n",
    "    template=cv2.imread(template_path, 0)\n",
    "    w,h=template.shape[::-1]\n",
    "    method=cv2.TM_CCOEFF_NORMED\n",
    "    threshold=0.9 \n",
    "    frame_count=0\n",
    "    detected_frame=None\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret,frame=cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "        tmpm=cv2.matchTemplate(frame_gray,template,method)\n",
    "        _,max_val,_,max_loc=cv2.minMaxLoc(tmpm)\n",
    "    \n",
    "        if max_val>=threshold:\n",
    "            top_left=max_loc\n",
    "            bottom_right=(top_left[0]+w, top_left[1]+h)\n",
    "            cv2.rectangle(frame,top_left,bottom_right,(0,255,0),3)\n",
    "            print(f\"Object detected in the frame {frame_count}\")\n",
    "            cv2.imwrite(output_path,frame)\n",
    "            detected_frame=frame.copy()  \n",
    "            break\n",
    "\n",
    "        frame_count +=1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "        \n",
    "    return detected_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(inputs, 'part2.mp4')  # Replace with your input video path\n",
    "template_path = os.path.join(inputs, 'template.png')  # Replace with your template image path\n",
    "output_path = os.path.join(outputs, 'detected_object.jpg')  # Output video path\n",
    "\n",
    "image = locate_object(video_path, template_path, output_path)\n",
    "display_images(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Implement a Kalman filter to predict the object's position in subsequent frames. **(5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track(video_path, template_path, output_path):\n",
    "    template=cv2.imread(template_path,0)\n",
    "    w,h= template.shape[::-1] \n",
    "    cap=cv2.VideoCapture(video_path)\n",
    "    method=cv2.TM_CCOEFF_NORMED\n",
    "    ret,frame=cap.read()\n",
    "\n",
    "    frame_height,frame_width=frame.shape[:2]\n",
    "    print('W :',frame_height,'H :',frame_width)\n",
    "    fps=cap.get(cv2.CAP_PROP_FPS)\n",
    "    fourcc=cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out=cv2.VideoWriter(output_path,fourcc,fps,(frame_width,frame_height))\n",
    " \n",
    "    kalman= cv2.KalmanFilter(4,2)\n",
    "    kalman.transitionMatrix=np.array([[1,0,1,0],[0,1,0,1],[0,0,1,0],[0,0,0,1]],np.float32)\n",
    "\n",
    "    kalman.measurementMatrix= np.eye(2, 4, dtype=np.float32)\n",
    "    kalman.measurementMatrix= np.array([[1,0,0,0],[0,1,0,0]],np.float32)\n",
    "\n",
    "    kalman.processNoiseCov=np.eye(4,dtype=np.float32)*0.8\n",
    "\n",
    "    initialized= False\n",
    "    frame_idx= 0\n",
    "    while cap.isOpened():\n",
    "        ret,frame=cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "    \n",
    "        frame_gray= cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        tmpm=cv2.matchTemplate(frame_gray, template,method) \n",
    "        _,_,max_val,max_loc= cv2.minMaxLoc(tmpm)\n",
    "\n",
    "        x,y =max_loc[0]+w//2,max_loc[1]+h//2\n",
    "        if not initialized:\n",
    "            kalman.statePre=np.array([[x],[y],[0],[0]], np.float32)\n",
    "            initialized= True\n",
    "        pred= kalman.predict()\n",
    "        mx,my= max_loc[0] + w//2, max_loc[1] + h//2\n",
    "        kalman.correct(np.array([[np.float32(mx)], [np.float32(my)]]))\n",
    "\n",
    "        cv2.rectangle(frame,max_loc,(max_loc[0]+w,max_loc[1]+h),(0,255,0),2)\n",
    "        cv2.circle(frame,(int(pred[0]),int(pred[1])),6,(0,0,255),-1)\n",
    "\n",
    "        out.write(frame)\n",
    "        frame_idx += 1\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(inputs, 'part2.mp4')  # Replace with your input video path\n",
    "template_path = os.path.join(inputs, 'template.png')  # Replace with your template image path\n",
    "output_path = os.path.join(outputs, 'tracked_object.mp4')  # Output video path\n",
    "\n",
    "track(video_path, template_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = display_video(output_path)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Compare Bayesian filtering and Kalman filtering (theoretically). **(2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO c): \n",
    "# Bayesian filters are broad class of algorithms that are used to compute probability distribution of states given noisy measurements,\n",
    "# while Kalman filter is a type of Bayesian filter that computes a point estimate(mean) and its covariance, not the whole distribution.\n",
    "# Since Bayesian filter estimates the full probability distribution of states, that makes it computationally more expensive than Kalman filter. \n",
    "# Kalman filter works well and efficient only when linearity and gaussian noise is present, while Bayesian works for any type of noise and linearity/non-linearity.\n",
    "# The above condition makes Bayesian more flexible to implement in various applications while Kalman filter is more suitable specific application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
